{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "What is Reinforcement Learning?\n",
    "\n",
    "Machine learning mainly consists of three methods: Supervised Learning, Unsupervised Learning and Reinforcement Learning. Supervised Learning provides mapping functionality between input and output using labelled dataset. \n",
    "Some of the supervised learning methods:\n",
    "Linear Regression,\n",
    "Support Vector Machines,\n",
    "Nueral Network, etc.\n",
    "\n",
    "Unsupervised Learning provides grouping and clustering functionality.\n",
    "Some of the unsupervised learning methods: \n",
    "K-Means, \n",
    "DBScan, etc.\n",
    "\n",
    "Reinforcement Learning is different from supervised and unsupervised learning. \n",
    "RL provides behaviour learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "\"A reinforcement learning algorithm, or agent, learns by interacting with its environment. The agent receives rewards by performing correctly and penalties for performing incorrectly. The agent learns without intervention from a human by maximizing its reward and minimizing its penalty\" *. RL agents are used in different applications: Robotics, self driving cars, playing atari games, managing investment portfolio, \n",
    "\n",
    "control problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 21px;\">\n",
    "Markov Decision Process:\n",
    "It consists of five tuples: status, actions, rewards, state transition probability, discount factor.\n",
    "\n",
    "Markov decision processes formally describe an environment for reinforcement learning.\n",
    "\n",
    "There are 3 techniques for solving MDPs: Dynamic Programming (DP) Learning, Monte Carlo (MC) Learning, Temporal Difference (TD) Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 21px;\">\n",
    "Markov Property :\n",
    "A state St is Markov if and only if P[St+1|St] =P[St+1|S1,...,St]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 21px;\">\n",
    "RL Components:\n",
    "\n",
    "Rewards:\n",
    "\n",
    "A reward Rt is a scalar feedback signal.\n",
    "\n",
    "The agent’s job is to maximise cumulative reward\n",
    "\n",
    "State Transition Probability:\n",
    "\n",
    "p(s′,r|s,a).= Pr{St=s′,Rt=r|St-1=s,At−1=a},\n",
    "\n",
    "Discount Factor:\n",
    "\n",
    "The discount γ∈[0,1] is the present value of future rewards.\n",
    "\n",
    "Return:\n",
    "\n",
    "The return Gt is the total discounted reward from time-step t.\n",
    "\n",
    "Value Function:\n",
    "\n",
    "Value function is a prediction of future reward. How good is each state and/or action.\n",
    "\n",
    "The value function v(s) gives the long-term value of state s\n",
    "\n",
    "Vπ(s) =Eπ[Rt+1+γRt+2+γ2Rt+3+...|St=s]\n",
    "\n",
    "Value function has two parts: immediate reward and discounted value of successor state.\n",
    "\n",
    "Policy (π):\n",
    "\n",
    "A policy is the agent’s behaviour. It is a map from state to action.\n",
    "\n",
    "Deterministic policy: a=π(s).\n",
    "\n",
    "Stochastic policy: π(a|s) =P[At=a|St=s].\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
