{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "What is Reinforcement Learning?\n",
    "\n",
    "Machine learning mainly consists of three methods: Supervised Learning, Unsupervised Learning and Reinforcement Learning. Supervised Learning provides mapping functionality between input and output using labelled dataset. \n",
    "Some of the supervised learning methods:\n",
    "Linear Regression,\n",
    "Support Vector Machines,\n",
    "Nueral Network, etc.\n",
    "\n",
    "Unsupervised Learning provides grouping and clustering functionality.\n",
    "Some of the unsupervised learning methods: \n",
    "K-Means, \n",
    "DBScan, etc.\n",
    "\n",
    "Reinforcement Learning is different from supervised and unsupervised learning. \n",
    "RL provides behaviour learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "\"A reinforcement learning algorithm, or agent, learns by interacting with its environment. The agent receives rewards by performing correctly and penalties for performing incorrectly. The agent learns without intervention from a human by maximizing its reward and minimizing its penalty\" *. RL agents are used in different applications: Robotics, self driving cars, playing atari games, managing investment portfolio, \n",
    "\n",
    "control problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 21px;\">\n",
    "Markov Decision Process:\n",
    "It consists of five tuples: status, actions, rewards, state transition probability, discount factor.\n",
    "\n",
    "Markov decision processes formally describe an environment for reinforcement learning.\n",
    "\n",
    "There are 3 techniques for solving MDPs: Dynamic Programming (DP) Learning, Monte Carlo (MC) Learning, Temporal Difference (TD) Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 21px;\">\n",
    "Markov Property :\n",
    "A state St is Markov if and only if P[St+1|St] =P[St+1|S1,...,St]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 21px;\">\n",
    "RL Components:\n",
    "\n",
    "Rewards:\n",
    "\n",
    "A reward Rt is a scalar feedback signal.\n",
    "\n",
    "The agent’s job is to maximise cumulative reward\n",
    "\n",
    "State Transition Probability:\n",
    "\n",
    "p(s′,r|s,a).= Pr{St=s′,Rt=r|St-1=s,At−1=a},\n",
    "\n",
    "Discount Factor:\n",
    "\n",
    "The discount γ∈[0,1] is the present value of future rewards.\n",
    "\n",
    "Return:\n",
    "\n",
    "The return Gt is the total discounted reward from time-step t.\n",
    "\n",
    "Value Function:\n",
    "\n",
    "Value function is a prediction of future reward. How good is each state and/or action.\n",
    "\n",
    "The value function v(s) gives the long-term value of state s\n",
    "\n",
    "Vπ(s) = Eπ[Rt+1+γRt+2+γ2Rt+3+...|St=s]\n",
    "\n",
    "Value function has two parts: immediate reward and discounted value of successor state.\n",
    "\n",
    "Policy (π):\n",
    "\n",
    "A policy is the agent’s behaviour. It is a map from state to action.\n",
    "\n",
    "Deterministic policy: a = π(s).\n",
    "\n",
    "Stochastic policy: π(a|s) = P[At=a|St=s].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 21px;\">\n",
    "The state value function Vπ(s) of an MDP is the expected return starting from state s, and then following policy π\n",
    "\n",
    "Vπ(s) = Vπ(s) = Eπ[ Rt+1 + γRt+2 + γ2Rt+3 +...|St=s]\n",
    "\n",
    "The action value function qπ(s, a) is the expected return starting from state s, taking action a, and the following policy π\n",
    "\n",
    "qπ(s, a) = Eπ[Gt|St=s, At = a]\n",
    "\n",
    "Optimal Value Functions:\n",
    "\n",
    "The optimal state value function V*(s) is the maximum value function over all policies\n",
    "\n",
    "V*(s) = max π Vπ(s),\n",
    "\n",
    "The optimal action value function q*(s, a) is the maximum action value function over all policies\n",
    "\n",
    "q*(s, a) = max π qπ(s, a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 21px;\">\n",
    "Planning vs RL :\n",
    "\n",
    "Planning:\n",
    "\n",
    "Rules of the game are known.\n",
    "\n",
    "A model of the environment is known.\n",
    "\n",
    "The agent performs computations with its mode.\n",
    "\n",
    "The agent improves its policy.\n",
    "\n",
    "RL:\n",
    "\n",
    "The environment is initially unknown.\n",
    "\n",
    "The agent interacts with the environment.\n",
    "\n",
    "The agent improves its policy.\n",
    "\n",
    "Exploration and Exploitation :\n",
    "\n",
    "Reinforcement learning is like trial-and-error learning.\n",
    "\n",
    "The agent should discover a good policy.\n",
    "\n",
    "Exploration finds more information about the environment (Gather more information).\n",
    "\n",
    "Exploitation exploits known information to maximise reward (Make the best decision given current information).\n",
    "\n",
    "Prediction & Control Problem (Pattern of RL algorithms) :\n",
    "\n",
    "Prediction: evaluate the future (Finding value given a policy).\n",
    "\n",
    "Control: optimise the future (Finding optimal/best policy).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
